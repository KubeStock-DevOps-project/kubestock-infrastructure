# KubeStock Infrastructure - Component Checklist

## ‚úÖ COMPLETED COMPONENTS

### 0. Backend Configuration
- ‚úÖ `backend.tf` with S3 backend configuration
  - Bucket: `kubestock-terraform-state` (placeholder)
  - Key: `terraform.tfstate`
  - Region: `us-east-1`

### 1. Networking (3-AZ HA)
- ‚úÖ 1 VPC (`kubestock-vpc`)
- ‚úÖ 3 Public Subnets (us-east-1a, us-east-1b, us-east-1c)
- ‚úÖ 3 Private Subnets (us-east-1a, us-east-1b, us-east-1c)
- ‚úÖ 1 Internet Gateway
- ‚úÖ 1 NAT Gateway (in us-east-1a for cost optimization)
- ‚úÖ 1 Public Route Table (routes 0.0.0.0/0 to IGW)
- ‚úÖ 3 Private Route Tables (all route 0.0.0.0/0 to single NAT Gateway)

### 2. Compute & Access (Non-HA, Low-Cost)
- ‚úÖ 1 EC2 Key Pair (`kubestock-key`)
- ‚úÖ 1 Elastic IP (for Bastion host)
- ‚úÖ 1 EC2 Instance (Bastion): t3.micro in public subnet - for SSH access and port forwarding
- ‚úÖ 1 EC2 Instance (Dev Server): t3.medium in public subnet - for VS Code, Terraform, Ansible
  - No Elastic IP (uses free dynamic public IP)
  - Stop when not in use - costs $0 when stopped
- ‚úÖ 1 EC2 Instance (Control Plane): t3.medium in private subnet (us-east-1a)
- ‚úÖ 1 EC2 Launch Template (Workers): t3.large configuration
- ‚úÖ 1 EC2 Auto Scaling Group (Workers):
  - Configured across all 3 private subnets
  - min_size = 1, desired_capacity = 1, max_size = 3

### 3. Security Groups
- ‚úÖ `sg_bastion`: Port 22 from my IP
- ‚úÖ `sg_k8s_nodes`: All internal traffic + Port 22 from bastion + Port 6443 from NLB
- ‚úÖ `sg_rds`: Port 5432 from K8s nodes only
- ‚úÖ `sg_nlb_api`: Port 6443 from bastion and my IP

### 4. Managed Services (Non-HA, Low-Cost)
- ‚úÖ 1 AWS RDS Instance (PostgreSQL 16):
  - Instance: db.t4g.medium
  - **multi_az = false** (Single-AZ)
  - Storage: 20GB (auto-scales to 100GB)
- ‚úÖ 1 RDS DB Subnet Group (using all 3 private subnets)
- ‚úÖ 1 AWS Cognito User Pool
- ‚úÖ 1 AWS Cognito User Pool Client
- ‚úÖ 1 AWS Network Load Balancer:
  - Spans all 3 public subnets
  - Listens on Port 6443
  - Points to single Control Plane instance

### 5. IAM
- ‚úÖ 1 IAM Role (`kubestock-node-role`)
- ‚úÖ 1 IAM Policy with permissions for:
  - EC2 and AutoScaling (Cluster Autoscaler)
  - EBS (EBS CSI Driver)
  - ELB (AWS Load Balancer Controller)
- ‚úÖ 1 IAM Instance Profile (`kubestock-node-profile`)

---

## üìã ARCHITECTURE HIGHLIGHTS

### Network: 3-AZ HA ‚úì
- Full high-availability network spanning 3 availability zones
- 3 public subnets for internet-facing resources
- 3 private subnets for secure backend resources
- Single NAT Gateway for cost optimization (can scale to 3 for full HA NAT)

### Compute: Non-HA "Sprint" Configuration ‚úì
- **Cost-Optimized**: Single control plane, single worker to start
- **HA-Ready**: Worker ASG spans all 3 AZs, ready to scale
- **Upgrade Path**: Can easily add more control planes and workers

### Database: Single-AZ ‚úì
- **Cost-Optimized**: Single-AZ RDS instance
- **HA-Ready**: Subnet group spans 3 AZs, can enable Multi-AZ later

---

## üéØ RESOURCE NAMING CONVENTION

All resources follow the pattern: `kubestock-<resource-type>`

Examples:
- VPC: `kubestock-vpc`
- Subnets: `kubestock-public-subnet-us-east-1a`
- Security Groups: `kubestock-sg-bastion`
- Instances: `kubestock-control-plane`
- IAM Role: `kubestock-node-role`

---

## üè∑Ô∏è TAGGING STRATEGY

All resources tagged with:
- `Project = "KubeStock"`
- `Environment = "production"`
- `ManagedBy = "Terraform"`

Additional role-specific tags for K8s resources:
- `kubernetes.io/cluster/kubestock = "owned"`
- `k8s.io/cluster-autoscaler/kubestock = "owned"`
- `k8s.io/cluster-autoscaler/enabled = "true"`

---

## üì¶ FILES CREATED

1. ‚úÖ `backend.tf` - S3 backend configuration
2. ‚úÖ `main.tf` - Terraform and provider configuration
3. ‚úÖ `variables.tf` - Variable definitions with defaults
4. ‚úÖ `network.tf` - VPC, subnets, NAT, IGW, route tables, security groups
5. ‚úÖ `compute.tf` - EC2 key pair, bastion, control plane, workers
6. ‚úÖ `iam.tf` - IAM roles, policies, instance profile
7. ‚úÖ `database.tf` - RDS PostgreSQL with single-AZ
8. ‚úÖ `cognito.tf` - Cognito user pool and client
9. ‚úÖ `load_balancer.tf` - Network Load Balancer for K8s API
10. ‚úÖ `outputs.tf` - All important outputs
11. ‚úÖ `terraform.tfvars.example` - Example variables file
12. ‚úÖ `README.md` - Complete documentation
13. ‚úÖ `.gitignore` - Ignore sensitive files

---

## üí∞ COST OPTIMIZATION vs FULL HA

### Current Configuration (Cost-Optimized)
- 1 NAT Gateway
- 1 Bastion (t3.micro) - always running
- 1 Dev Server (t3.medium) - **stop when not in use** 
- 1 Control Plane (t3.medium)
- 1 Worker (t3.large)
- 1 Single-AZ RDS (db.t4g.medium)

### Estimated Monthly Costs
**If Dev Server Running 24/7**: ~$235/month
- NAT Gateway: $32
- Bastion: $7
- Dev Server: $30 (running)
- Control Plane: $30
- Worker: $60
- RDS: $50
- NLB: $16
- Other: $10

**If Dev Server Stopped (Recommended)**: ~$206/month
- Dev Server when stopped: ~$1-2 (storage only)
- **Save ~$28/month** by stopping dev server when not in use

### Full HA Configuration
- 3 NAT Gateways (+$64/month)
- 3 Control Planes (+$60/month)
- 3 Workers (+$120/month)
- 1 Multi-AZ RDS (+$40/month)

**Full HA Monthly Cost**: ~$500/month
**Current Savings**: ~$294/month

---

## üöÄ NEXT STEPS

1. Update `backend.tf` with your actual S3 bucket name
2. Create `terraform.tfvars` from the example
3. Generate SSH key pair
4. Run `terraform init`
5. Run `terraform plan`
6. Run `terraform apply`
7. Access bastion and begin Kubernetes installation

---

**Status**: ‚úÖ ALL COMPONENTS COMPLETED  
**Date**: 2025-11-13  
**Project**: KubeStock Infrastructure
